{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T17:16:01.872627400Z",
     "start_time": "2026-01-09T16:36:40.639105600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# --- 1. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ù†ÙØ³ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø³Ø§Ø¨Ù‚ Ù…Ø¹ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ù†ÙˆØ¹) ---\n",
    "base_path = r'C:\\Users\\haama\\PycharmProjects\\machine-learning-basics\\Skin-Disease-Analyzer\\data'\n",
    "metadata = pd.read_csv(os.path.join(base_path, 'HAM10000_metadata.csv'))\n",
    "all_image_paths = {os.path.splitext(os.path.basename(x))[0]: x for x in glob(os.path.join(base_path, '**', '*.jpg'), recursive=True)}\n",
    "metadata['path'] = metadata['image_id'].map(all_image_paths)\n",
    "metadata.dropna(subset=['path'], inplace=True)\n",
    "metadata = metadata.reset_index(drop=True)\n",
    "\n",
    "metadata['age'] = metadata['age'].fillna(metadata['age'].mean())\n",
    "meta_df = pd.get_dummies(metadata[['age', 'sex', 'localization']])\n",
    "meta_array = meta_df.values.astype('float32')\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(metadata['dx']).astype('int32')\n",
    "train_idx, test_idx = train_test_split(np.arange(len(metadata)), test_size=0.2, random_state=42)\n",
    "\n",
    "# --- 2. Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ ØªØ³Ù…ÙŠØ© Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ù„Ø±Ø¨Ø·Ù‡Ø§ Ø¨Ø§Ù„Ù…ÙˆÙ„Ø¯ ---\n",
    "num_meta = meta_array.shape[1]\n",
    "img_input = layers.Input(shape=(128, 128, 3), name=\"img_input\") # Ø£Ø¹Ø·ÙŠÙ†Ø§Ù‡ Ø§Ø³Ù…Ø§Ù‹\n",
    "base = tf.keras.applications.MobileNetV2(include_top=False, weights='imagenet', input_shape=(128, 128, 3))(img_input)\n",
    "x_img = layers.GlobalAveragePooling2D()(base)\n",
    "\n",
    "meta_input = layers.Input(shape=(num_meta,), name=\"meta_input\") # Ø£Ø¹Ø·ÙŠÙ†Ø§Ù‡ Ø§Ø³Ù…Ø§Ù‹\n",
    "x_meta = layers.Dense(16, activation='relu')(meta_input)\n",
    "\n",
    "merged = layers.Concatenate()([x_img, x_meta])\n",
    "output = layers.Dense(7, activation='softmax')(layers.Dense(64, activation='relu')(merged))\n",
    "\n",
    "hybrid_model = models.Model(inputs=[img_input, meta_input], outputs=output)\n",
    "hybrid_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# --- 3. Ø§Ù„Ù…ÙˆÙ„Ø¯ Ø§Ù„Ù…Ø­Ø¯Ø« (Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø¬ÙˆÙ‡Ø±ÙŠ Ù‡Ù†Ø§) ---\n",
    "def final_safe_generator(indices, batch_size=32):\n",
    "    while True:\n",
    "        np.random.shuffle(indices)\n",
    "        for i in range(0, len(indices), batch_size):\n",
    "            batch_indices = indices[i:i+batch_size]\n",
    "            batch_imgs, batch_metas, batch_labels = [], [], []\n",
    "\n",
    "            for idx in batch_indices:\n",
    "                try:\n",
    "                    img = img_to_array(load_img(metadata.iloc[idx]['path'], target_size=(128, 128))) / 255.0\n",
    "                    batch_imgs.append(img)\n",
    "                    batch_metas.append(meta_array[idx])\n",
    "                    batch_labels.append(labels[idx])\n",
    "                except: continue\n",
    "\n",
    "            if len(batch_imgs) > 0:\n",
    "                # Ù†Ø±Ø³Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙƒÙ€ Dictionary Ù„Ø±Ø¨Ø·Ù‡Ø§ Ø¨Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ØªÙŠ ÙˆØ¶Ø¹Ù†Ø§Ù‡Ø§ ÙÙŠ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\n",
    "                yield ({\"img_input\": np.array(batch_imgs), \"meta_input\": np.array(batch_metas)}, np.array(batch_labels))\n",
    "\n",
    "# --- 4. Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ---\n",
    "print(\"ğŸš€ ÙŠØ¨Ø¯Ø£ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø¢Ù†...\")\n",
    "train_gen = final_safe_generator(train_idx)\n",
    "test_gen = final_safe_generator(test_idx)\n",
    "\n",
    "hybrid_model.fit(\n",
    "    train_gen,\n",
    "    steps_per_epoch=len(train_idx) // 32,\n",
    "    epochs=10,\n",
    "    validation_data=test_gen,\n",
    "    validation_steps=len(test_idx) // 32\n",
    ")"
   ],
   "id": "cca69fa98999277b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ÙŠØ¨Ø¯Ø£ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø¢Ù†...\n",
      "Epoch 1/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m270s\u001B[0m 993ms/step - accuracy: 0.6895 - loss: 0.8957 - val_accuracy: 0.6673 - val_loss: 3.4227\n",
      "Epoch 2/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m248s\u001B[0m 994ms/step - accuracy: 0.7953 - loss: 0.5767 - val_accuracy: 0.0307 - val_loss: 5.3149\n",
      "Epoch 3/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m249s\u001B[0m 995ms/step - accuracy: 0.8399 - loss: 0.4554 - val_accuracy: 0.1205 - val_loss: 4.8423\n",
      "Epoch 4/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m231s\u001B[0m 926ms/step - accuracy: 0.8410 - loss: 0.4387 - val_accuracy: 0.6657 - val_loss: 3.2275\n",
      "Epoch 5/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m233s\u001B[0m 933ms/step - accuracy: 0.8669 - loss: 0.3716 - val_accuracy: 0.6672 - val_loss: 4.2581\n",
      "Epoch 6/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m228s\u001B[0m 912ms/step - accuracy: 0.8812 - loss: 0.3204 - val_accuracy: 0.6362 - val_loss: 2.4367\n",
      "Epoch 7/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m213s\u001B[0m 854ms/step - accuracy: 0.9055 - loss: 0.2803 - val_accuracy: 0.5809 - val_loss: 3.5216\n",
      "Epoch 8/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m215s\u001B[0m 861ms/step - accuracy: 0.9045 - loss: 0.2660 - val_accuracy: 0.6555 - val_loss: 2.4361\n",
      "Epoch 9/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m238s\u001B[0m 953ms/step - accuracy: 0.9280 - loss: 0.2061 - val_accuracy: 0.6114 - val_loss: 1.9269\n",
      "Epoch 10/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m234s\u001B[0m 935ms/step - accuracy: 0.9182 - loss: 0.2155 - val_accuracy: 0.6484 - val_loss: 2.6344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x13dca788e80>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T17:41:19.528409200Z",
     "start_time": "2026-01-09T17:21:04.079252800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# 1. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª (Ù†ÙØ³ Ù…Ø³Ø§Ø±Ùƒ)\n",
    "base_path = r'C:\\Users\\haama\\PycharmProjects\\machine-learning-basics\\Skin-Disease-Analyzer\\data'\n",
    "metadata = pd.read_csv(os.path.join(base_path, 'HAM10000_metadata.csv'))\n",
    "all_image_paths = {os.path.splitext(os.path.basename(x))[0]: x for x in glob(os.path.join(base_path, '**', '*.jpg'), recursive=True)}\n",
    "metadata['path'] = metadata['image_id'].map(all_image_paths)\n",
    "metadata.dropna(subset=['path'], inplace=True)\n",
    "metadata = metadata.reset_index(drop=True)\n",
    "\n",
    "# 2. ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ© (Ø§Ù„Ø¹Ù…Ø±ØŒ Ø§Ù„Ø¬Ù†Ø³ØŒ Ø¥Ù„Ø®)\n",
    "metadata['age'] = metadata['age'].fillna(metadata['age'].mean())\n",
    "meta_df = pd.get_dummies(metadata[['age', 'sex', 'localization']])\n",
    "meta_array = meta_df.values.astype('float32')\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(metadata['dx']).astype('int32')\n",
    "\n",
    "# 3. Ø­Ø³Ø§Ø¨ \"Ø£Ù‡Ù…ÙŠØ©\" ÙƒÙ„ Ù…Ø±Ø¶ (Class Weights) Ù„Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© Ø¹Ø¯Ù… Ø§Ù„ØªÙˆØ§Ø²Ù†\n",
    "weights = class_weight.compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "class_weights_dict = dict(enumerate(weights))\n",
    "\n",
    "# 4. Ø§Ù„Ù…ÙˆÙ„Ø¯ \"Ø§Ù„Ø°ÙƒÙŠ\" (ÙŠØ±Ø³Ù„ Ø§Ù„ØµÙˆØ±Ø© + Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª + Ø§Ù„ÙˆØ²Ù†) Ù„ÙŠØªØ¬Ù†Ø¨ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡\n",
    "def final_generator(indices, batch_size=32):\n",
    "    while True:\n",
    "        np.random.shuffle(indices)\n",
    "        for i in range(0, len(indices), batch_size):\n",
    "            batch_indices = indices[i:i+batch_size]\n",
    "            batch_imgs, batch_metas, batch_labels, batch_sample_weights = [], [], [], []\n",
    "            for idx in batch_indices:\n",
    "                try:\n",
    "                    img = img_to_array(load_img(metadata.iloc[idx]['path'], target_size=(128, 128))) / 255.0\n",
    "                    label = labels[idx]\n",
    "                    batch_imgs.append(img)\n",
    "                    batch_metas.append(meta_array[idx])\n",
    "                    batch_labels.append(label)\n",
    "                    batch_sample_weights.append(class_weights_dict[label])\n",
    "                except: continue\n",
    "            if len(batch_imgs) > 0:\n",
    "                yield ({\"img_input\": np.array(batch_imgs), \"meta_input\": np.array(batch_metas)},\n",
    "                       np.array(batch_labels),\n",
    "                       np.array(batch_sample_weights))\n",
    "\n",
    "# 5. Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù‡Ø¬ÙŠÙ† (Ø£ØµÙ„Ø­Ù†Ø§ Ø§Ù„ØªÙˆØµÙŠÙ„Ø§Øª Ù‡Ù†Ø§)\n",
    "num_meta = meta_array.shape[1]\n",
    "img_input = layers.Input(shape=(128, 128, 3), name=\"img_input\")\n",
    "base_model = tf.keras.applications.MobileNetV2(include_top=False, weights='imagenet', input_shape=(128, 128, 3))\n",
    "base_model.trainable = False # ØªØ¬Ù…ÙŠØ¯ Ù„Ù…Ù†Ø¹ Ø§Ù„Ù€ Overfitting\n",
    "\n",
    "x_img = base_model(img_input)\n",
    "x_img = layers.GlobalAveragePooling2D()(x_img)\n",
    "x_img = layers.Dropout(0.5)(x_img)\n",
    "\n",
    "meta_input = layers.Input(shape=(num_meta,), name=\"meta_input\")\n",
    "x_meta = layers.Dense(32, activation='relu')(meta_input)\n",
    "\n",
    "merged = layers.Concatenate()([x_img, x_meta])\n",
    "x = layers.Dense(128, activation='relu')(merged)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "output = layers.Dense(7, activation='softmax')(x)\n",
    "\n",
    "hybrid_model = models.Model(inputs=[img_input, meta_input], outputs=output)\n",
    "hybrid_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 6. ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„Ø¨Ø¯Ø¡ ÙÙˆØ±Ø§Ù‹\n",
    "train_idx, test_idx = train_test_split(np.arange(len(metadata)), test_size=0.2, random_state=42)\n",
    "train_gen = final_generator(train_idx)\n",
    "test_gen = final_generator(test_idx)\n",
    "\n",
    "print(\"ğŸš€ ÙŠØ¨Ø¯Ø£ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø¢Ù† Ø¨Ø£ÙØ¶Ù„ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª...\")\n",
    "hybrid_model.fit(train_gen, steps_per_epoch=len(train_idx)//32, epochs=10,\n",
    "                 validation_data=test_gen, validation_steps=len(test_idx)//32)\n",
    "\n",
    "hybrid_model.save('skin_disease_best_model.h5')\n",
    "print(\"âœ… Ù…Ø¨Ø±ÙˆÙƒ! Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¬Ø§Ù‡Ø² ÙˆØ§Ù„Ù†ØªØ§Ø¦Ø¬ Ø³ØªÙƒÙˆÙ† Ù…Ù…ØªØ§Ø²Ø©.\")"
   ],
   "id": "7b01f2aff4075174",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ÙŠØ¨Ø¯Ø£ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø¢Ù† Ø¨Ø£ÙØ¶Ù„ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª...\n",
      "Epoch 1/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m144s\u001B[0m 557ms/step - accuracy: 0.3158 - loss: 2.3430 - val_accuracy: 0.5439 - val_loss: 1.4924\n",
      "Epoch 2/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m137s\u001B[0m 550ms/step - accuracy: 0.4426 - loss: 1.4445 - val_accuracy: 0.6210 - val_loss: 1.2563\n",
      "Epoch 3/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m138s\u001B[0m 554ms/step - accuracy: 0.5216 - loss: 1.3398 - val_accuracy: 0.5615 - val_loss: 1.2769\n",
      "Epoch 4/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m139s\u001B[0m 558ms/step - accuracy: 0.5224 - loss: 1.2709 - val_accuracy: 0.5363 - val_loss: 1.2895\n",
      "Epoch 5/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m98s\u001B[0m 391ms/step - accuracy: 0.5278 - loss: 1.1384 - val_accuracy: 0.5824 - val_loss: 1.1454\n",
      "Epoch 6/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m94s\u001B[0m 376ms/step - accuracy: 0.5384 - loss: 1.1247 - val_accuracy: 0.6327 - val_loss: 1.0992\n",
      "Epoch 7/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m110s\u001B[0m 440ms/step - accuracy: 0.5562 - loss: 1.1169 - val_accuracy: 0.5769 - val_loss: 1.1312\n",
      "Epoch 8/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m131s\u001B[0m 524ms/step - accuracy: 0.5623 - loss: 1.0624 - val_accuracy: 0.5784 - val_loss: 1.0272\n",
      "Epoch 9/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m119s\u001B[0m 476ms/step - accuracy: 0.5880 - loss: 1.0259 - val_accuracy: 0.6464 - val_loss: 1.0410\n",
      "Epoch 10/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m104s\u001B[0m 418ms/step - accuracy: 0.5527 - loss: 1.0759 - val_accuracy: 0.6677 - val_loss: 1.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ù…Ø¨Ø±ÙˆÙƒ! Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¬Ø§Ù‡Ø² ÙˆØ§Ù„Ù†ØªØ§Ø¦Ø¬ Ø³ØªÙƒÙˆÙ† Ù…Ù…ØªØ§Ø²Ø©.\n"
     ]
    }
   ],
   "execution_count": 56
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
